#!/usr/bin/env python3
import re
import concurrent.futures
import datetime
import os
import sys
import hashlib
import functools
import itertools
import signal
import contextlib

chunks = itertools.count(0)

@contextlib.contextmanager
def timeout(seconds=1, message='timeout'):
    def fn(*_):
        raise Exception('%s after %s seconds' % (message, seconds)) from None
    signal.signal(signal.SIGALRM, fn)
    signal.alarm(seconds)
    try:
        yield
    except:
        raise
    finally:
        signal.alarm(0)


@functools.lru_cache()
def sequential_chunk_num(chunk): # otherwise there will be holes in the sequence because of best effort chunking
    return next(chunks)

def main():
    os.chdir(os.environ['BACKUP_ROOT'])
    name = datetime.datetime.utcnow().isoformat().split('.')[0] + 'Z.tar.lz4.gpg'
    stdin = (line.strip().split('\t') for line in sys.stdin)
    index = {path: {'blake2b': blake2b, 'tar': tar, 'size': size} for path, tar, blake2b, size in stdin}
    total_size = 0
    num_cpu = int(os.environ.get('BACKUP_NUMCPU', os.cpu_count()))
    with concurrent.futures.ProcessPoolExecutor(num_cpu) as pool:
        results = pool.map(hash_file, paths(), chunksize=500)
        results = filter(None, results)
        results = rate(results)
        for path, blake2b, size in results:
            if index.get(path, {}).get('blake2b') != blake2b:
                total_size += size
                chunk = total_size // (1024 * 1024 * int(os.environ.get('BACKUP_CHUNK_MEGABYTES', 100)))
                chunk = sequential_chunk_num(chunk)
                chunk = str(chunk).zfill(os.environ.get('BACKUP_PAD_ZEROS', 2))
                print('\t'.join([path, f'{name}.{chunk}', blake2b, str(size)]))
            else:
                ref = index[path]
                print('\t'.join([path, ref['tar'], ref['blake2b'], ref['size']]))

def rate(xs):
    count = 0
    for x in xs:
        yield x
        count += 1
        if count % 10_000 == 0:
            print(f'scanned {count} files', file=sys.stderr)
    print(f'scanned: {count} files', file=sys.stderr)

def paths():
    with open('.backup/ignore') as f:
        ignores = [x.strip() for x in f.read().splitlines() if x.strip()]
    for path, dirs, files in os.walk('.'):
        for file in files:
            file_path = os.path.join(path, file)
            if any(re.search(i, file_path) for i in ignores):
                continue
            yield file_path

def hash_file(path):
    @timeout(seconds=int(os.environ.get('BACKUP_TIMEOUT', 15)))
    def _hash():
        size = 0
        blake2b = hashlib.blake2b()
        try:
            with open(path, 'rb') as f:
                while True:
                    data = f.read(1024 ** 2)
                    if not data:
                        break
                    blake2b.update(data)
                    size += len(data)
        except (PermissionError, OSError) as e:
            print('skipping:', os.getcwd(), e, file=sys.stderr)
        else:
            blake2b = blake2b.hexdigest()
            return path, blake2b, size
    try:
        return _hash()
    except:
        print(f'skip: timeout hashing path: {path}', file=sys.stderr, flush=True)

if __name__ == '__main__':
    main()
