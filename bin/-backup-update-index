#!/usr/bin/env python3
# type: ignore
import io
import tarfile
import time
import re
import datetime
import os
import sys
import hashlib
import functools
import itertools
import signal
import contextlib
import subprocess

check_output = lambda *a: subprocess.check_output(' '.join(map(str, a)), shell=True, executable='/bin/bash').decode('utf-8').strip()

def main():
    os.chdir(os.environ['BACKUP_ROOT'])

    with open('.backup/index') as f:
        lines = [line.strip().split('\t') for line in f]
    index = {path: {'blake2b': blake2b, 'tar': tar, 'size': size, 'mode': mode} for path, tar, blake2b, size, mode in lines}

    commit_number = check_output(r'cd .backup && git log --pretty=%s | grep "\.tar\.lz4\.enc\." | wc -l').zfill(10)
    timestamp = datetime.datetime.now(datetime.UTC).isoformat().split('.')[0] + 'Z'
    name = f'{commit_number}.{timestamp}.tar.lz4.enc'
    chunk_megabytes = int(os.environ.get('BACKUP_CHUNK_MEGABYTES', '100'))
    total_size = 0
    times = []
    new_index = []
    new_data = {}
    historical_data = load_historical()

    # scan the filesystem
    for path in rate(paths()):

        symlink = os.path.islink(path)

        if symlink:

            try:
                os.stat(path)
            except FileNotFoundError:

                # only non broken links are supported
                print('skip:', path, '=>', os.readlink(path), file=sys.stderr)
                continue

            # the link target, relative to $BACKUP_ROOT, is stored instead of the blake2b hash. when the link changes, the index gets updated.
            blake2b = './' + os.path.relpath(os.path.realpath(path))

            # only symlinks within $BACKUP_ROOT are supported
            if '..' in blake2b.split('/'):
                print('skip:', path, '=>', blake2b, file=sys.stderr)
                continue

            size = 0
            mode = '-'

        else:
            start = time.time()
            try:
                data, blake2b, size, mode = hash_file(path)
            except Failed:
                print('failed:', path, file=sys.stderr)
                continue
            except Timeout:
                print('timeout:', path, file=sys.stderr)
                continue
            times.append((time.time() - start, path))

        # blake2b unchanged
        if index.get(path, {}).get('blake2b') == blake2b:
            existing = index[path]
            new_index.append('\t'.join([path, existing['tar'], existing['blake2b'], existing['size'], existing['mode']]))

        # blake2b changed
        else:

            if symlink:

                # symlinks are only commited to the index, not to tarballs
                tar = 'symlink'

            else:

                # deduped from historical data
                if blake2b in historical_data:
                    tar = historical_data[blake2b]

                # deduped from new data
                elif blake2b in new_data:
                    tar = new_data[blake2b]

                # write new data to tarball
                else:
                    total_size += size
                    chunk = str(sequential_chunk_num(total_size // (1024 * 1024 * chunk_megabytes))).zfill(5)
                    tar = f'{name}.{chunk}'
                    tarinfo = tarfile.TarInfo(name=blake2b)
                    tarinfo.size = size
                    tarball(tar).addfile(tarinfo, io.BytesIO(data))
                    new_data[blake2b] = tar

            # build new index
            new_index.append('\t'.join([path, tar, blake2b, str(size), mode]))

    # close tarballs
    for k, v in tarballs.items():
        v.close()

    # write updated index
    new_index.sort()
    with open('.backup/index', 'w') as f:
        for line in new_index:
            f.write(line + '\n')

    # print stats about slow to scan files
    times = list(sorted(times, key=lambda x: x[0]))
    print('slowest scanned files:', file=sys.stderr)
    for seconds, path in times[-10:]:
        print('', path, round(seconds, 3), 'seconds', file=sys.stderr)

def rate(xs):
    count = 0
    for x in xs:
        yield x
        count += 1
        if count % 1_000 == 0:
            print(f'scanned {count} files', file=sys.stderr)
    print(f'scanned: {count} files', file=sys.stderr)

def paths():
    count = 0
    with open('.backup/ignore') as f:
        ignores = [x.strip() for x in f.read().splitlines() if x.strip()]
    ignores = [re.compile(i).search for i in ignores]
    for path, dirs, files in os.walk('.'):
        def file_paths():
            for d in dirs:
                d = os.path.join(path, d)
                if os.path.islink(d):
                    yield d
            for f in files:
                yield os.path.join(path, f)
        for file_path in file_paths():
            if any(ignore(file_path) for ignore in ignores):
                continue
            yield file_path
            count += 1
    assert count, 'fatal: nothing found to backup under root: {BACKUP_ROOT}'.format(**os.environ)

@contextlib.contextmanager
def timeout(seconds=1, message='timeout'):
    def fn(*_):
        raise Timeout
    signal.signal(signal.SIGALRM, fn)
    signal.alarm(seconds)
    try:
        yield
    except:
        raise
    finally:
        signal.alarm(0)

class Failed(Exception):
    pass

class Timeout(Exception):
    pass

def mode_to_string(mode):
    return str(oct(mode))[-3:]

@timeout(seconds=int(os.environ.get('BACKUP_TIMEOUT', '15')))
def hash_file(path):
    try:
        with open(path, 'rb') as f:
            data = f.read()
        mode = os.stat(path).st_mode
    except (PermissionError, OSError):
        raise Failed
    else:
        size = len(data)
        blake2b = hashlib.blake2b(data).hexdigest()
        return data, blake2b, size, mode_to_string(mode)

def load_historical():
    historical = {}
    proc = subprocess.Popen(shell=True, executable='/bin/bash', stdout=subprocess.PIPE, args="git log -p index", cwd='.backup')
    while True:
        line = proc.stdout.readline().decode('utf-8')
        if not line:
            break
        if not line.startswith('+./'):
            continue
        line = line[1:]
        path, tar, blake2b, size, mode = line.strip().split('\t')
        if blake2b not in historical:
            historical[blake2b] = tar
        else:
            assert historical[blake2b] == tar, f'fatal: each hash should only ever be written to a tarball once: {historical[blake2b]} {tar}'
    assert proc.wait() == 0
    return historical

chunks = itertools.count(0)

@functools.lru_cache(maxsize=1_000_000_000)
def sequential_chunk_num(chunk_size):
    return next(chunks)

tarballs = {}

@functools.lru_cache(maxsize=1_000_000_000)
def tarball(name):
    name = f'.backup/{name}'
    t = tarfile.open(name, 'w')
    tarballs[name] = t
    return t

if __name__ == '__main__':
    main()
